title,company,location,description,url,source,h1b_eligible,eligibility_reason,search_date
128 Cloud Software Engineer 3,ARSIEM,"Annapolis Junction, Maryland","About ARSIEM Corporation

At ARSIEM Corporation we are committed to fostering a proven and trusted partnership with our government clients. We provide support to multiple agencies across the United States Government. ARSIEM has an experienced workforce of qualified professionals committed to providing the best possible support.

As demand increases, ARSIEM continues to provide reliable and cutting-edge technical solutions at the best value to our clients. That means a career packed with opportunities to grow and the ability to have an impact on every client you work with.

ARSIEM is currently looking for a Cloud Software Engineer skilled in DevOps, particularly as it relates to cloud solutioning and administration; able to design and implement cloud based solutions; maintain and secure Linux based operating systems as well as design, manage and secure data flow management utilizing NiFi. The position will support one of our Government clients in Annapolis Junction, MD.

Responsibilities
• Develops, maintains, and enhances complex and diverse Big Data Cloud systems based upon documented requirements.
• Directly contributes to all stages of backend processing, analyzing, and indexing.
• Provides expertise in Cloud Computing, Hadoop Ecosystem including implementing Java applications, Distributed Computing, Information Retrieval (IR), and Object Oriented Design.
• Works individually or as part of a team.
• Reviews and tests software components for adherence to the design requirements and documents test results.
• Resolves software problem reports.
• Utilizes software development and software design methodologies appropriate to the development environment.
• Provides specific input to the software components of system design to include hardware/software tradeoffs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components.

Minimum Qualifications
• Experience with cloud platforms like Amazon Web Services (AWS) and Microsoft Azure
• Knowledge in containerization using Dockers, Kubernetes, or other container orchestration tools
• Should be able to troubleshoot and resolve complex issues
• Good communication skills are essential for collaborating with cross functional teams and stakeholders
• (12) years' experience software engineering experience in programs and contracts of similar scope, type, and complexity is required. Bachelor's degree in Computer Science or related discipline from an accredited college or university is required; four (4) years of which must be in programs utilizing Big Data cloud technologies and/or Distributed Computing.
• (4) years of cloud software engineering experience on projects with similar Big Data systems may be substituted for a bachelor's degree.
• Master's in computer science or related discipline from an accredited college or university may be substituted for two (2) years of experience.
• Cloudera Certified Hadoop Developer certification may be substituted for one (1) year of Cloud experience.
• (2) years of Cloud and/or Distributed Computing Information Retrieval (IR).
• (1) year of experience with implementing code that interacts with implementation of Cloud Big Table.
• (1) year of experience with implementing code that interacts with implementation of Cloud Distributed File System.
• (1) year of experience with implementing complex MapReduce analytics.
• (1) year of experience with implementing code that interacts with Cloud Distributed Coordination Frameworks.
• (1) year of experience in architecting Cloud Computing solutions
• (1) year of experience in debugging problems with Cloud based Distributed Computing Frameworks
• (1) year of experience in managing multi-node Cloud based installation
• Utility Computing, Network Management, Virtualization (VMWare or VirtualBox), Cloud Computing
• Multi Node Management and Installation: Management and installation of Cloud and Distributed Computing on multiple nodes, Python, CFEngine, Bash, Ruby or related technologies.
• Securing Cloud Based and Distributed applications through industry standard techniques such as Firewalls, PKI Certificate and Server Authentication with experience in corporate authentication service(s)
• Object Oriented Design and Programming, Java, Eclipse or similar development environment, MAVEN, RESTful web services.
• Cloud and Distributed Computing Technologies: at least one or a combination of several of the following areas: YARN, J2EE, MapReduce, Zookeeper, HDFS, HBase, JMS, Concurrent Programming, Multi-Node implementation/installation and other applicable technologies.
• At least one or a combination of several of the following areas: HDFS, HBASE, Apache Lucene, Apache Solr, MongoDB
• Ingesting, Parsing and Analysis of Disparate Data sources and formats: XML, JSON, CSV, Binary Formats, Sequence or Map Files, Avro and related technologies
• Aspect Oriented Design and Development
• Debugging and Profiling Cloud and Distributed Installations: Java Virtual Machine (JVM) memory management, Profiling Java Applications
• UNIX/LINUX, CentOS
• Experience with at least one SIGINT collection discipline areas (FORNSAT, CABLE, Terrestrial/Microwave, Overhead, and ELINT)
• Geolocation, emitter identification, and signal applications.
• Joint program collection platforms and dataflow architectures; signals characterization analysis
• CentOS, Linux/RedHat
• Configuration management tools such as Subversion, ClearQuest, or Razor

Preferred Qualifications
• Familiarity with Agile development methodologies like Scrum is beneficial
• Experience with the Elastic stack solutioning including indexing, searching and managing data
• Familiarity with Niagarafiles (NIFI) management
• Ansible scripting

Optional Qualifications
• Provide in-depth knowledge of Information Retrieval; assisting the software development team in designing, developing and testing Cloud Information Retrieval
• Implement complex workflows that manage Cloud MapReduce analytics
• Implement code that interacts with Cloud Distributed Coordination Frameworks
• Oversee one or more software development tasks and ensures the work is completed in accordance with the constraints of the software development process being used on any particular project Make recommendations for improving documentation and software development process standards
• Serve as a subject matter expert for Cloud Computing and corresponding technologies including Hadoop assisting the software development team in designing, developing and testing Cloud Computing Systems
• Debug problems with Cloud based Distributed Computing Frameworks
• Manage multi-node Cloud based installation
• Delegate programming and testing responsibilities to one or more teams and monitor their performance
• Select the software development process in coordination with the customer and system engineering
• Recommend new technologies and processes for complex cloud software projects
• Ensure quality control of all developed and modified software (U) Architect solutions to complex Cloud Software Engineering Problems such as efficiently processing and retrieving large amounts of data
• Make recommendations for improving documentation and software development process standards

$189,000 - $211,000 a year
The ARSIEM pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) job responsibilities, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other laws.

Benefits:
For an overview of our benefits, please visit our benefits tab.
https://www.arsiem.com/careers/

Original Posting Date:
2025-09-10

Clearance Requirement: This position requires an active TS/SCI with a polygraph. You must be a U.S. Citizen for consideration.

Candidate Referral: Do you know someone who would be GREAT at this role? If you do, ARSIEM has a way for you to earn a bonus through our referral program for persons presenting NEW (not in our resume database) candidates who are successfully placed on one of our projects. The bonus for this position is $10,000, and the referrer is eligible to receive the sum for any applicant we place within 12 months of referral. The bonus is paid after the referred employee reaches 6 months of employment.

ARSIEM is proud to be an Equal Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age, or any other federally protected class.",https://jobs.lever.co/arsiem/01effbcd-07f3-4858-86d2-67b801f24389?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic,JSearch,True,"The job posting does not contain any explicit requirements or implicit restrictions that would exclude H1B visa holders. There are no statements indicating that only U.S. citizens or permanent residents are eligible, nor does it mention that visa sponsorship is not available. Therefore, it can be inferred that H1B visa holders are eligible for this position.",2025-12-28 12:44
Big data Data Engineer - Global Data Analytics,Atria Group LLC,"Silver Spring, Maryland","Company Description

We specialize in Staffing, Consulting, Software Development, and Training along with IT services to small to medium size companies. AG's primary objective is to help companies maximize their IT resources and meet the ever-changing IT needs and challenges.

In addition, AG offers enterprise resource planning and enterprise application integration, supply-chain management, e-commerce solutions, and B2B public exchanges and B2B process integration solutions. Our company provides application analysis, design, development and programming, software engineering, systems development, testing, integration, and implementation, and management consulting services to various clients - including governmental agencies and private companies - throughout the United States and India.

We provide these services in multiple computing environments and use technologies such as client/server architecture, object-oriented programming languages and tools, distributed database management systems, state-of-the-art networking, and communications infrastructures. Our honest and realistic approach to recruiting dictates that AG does not entice or lure engineers from their employers. We represent only high caliber technical professionals who have committed to making a change required by career.

Job Description

Must Have Skills -

Overall 10+ years' experience in Datawarehousing related technologies.

3+ years architecting and managing AWS Big Data products and services such as EMR, RedShift, Data Pipeline and Kinesis

3+ years of working experience with Hadoop-based technologies such as MapReduce, Hive/Pig/Impala and NoSQL Databases

3+ years of extensive working knowledge in different programming or scripting languages like Java, Linux, C++, PHP, Ruby, Python and/or R.

Experience working with unstructured, semi-structured and unstructured data sets including social, weblogs and real-time data feeds

Proficient in designing efficient and robust ETL/ELT workflows

Able to tune Big Data solutions to improve performance and end-user experience

Bachelor's or Master's degree in computer science or software engineering

Knowledge BI and Visualization tools such as MicroStrategy/Tableau is a plus

Experience in the media industry is a plus

Must have the legal right to work in the United States

Additional Information

GOOD COMMUNICATION SKILLS

DURATION: 6 Months

INTERVIEW: PHONE","https://www.ziprecruiter.com/c/Atria-Group-LLC/Job/Big-data-Data-Engineer-Global-Data-Analytics/-in-Silver-Spring,MD?jid=7b5bd2d5926ca00b&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",JSearch,True,"The job posting does not contain any explicit or implicit restrictions regarding visa sponsorship or work authorization. There are no statements indicating that only U.S. citizens or Green Card holders are eligible, nor does it mention that no sponsorship is available. Therefore, H1B visa holders are likely eligible for this position.",2025-12-28 12:44
Sr. Cloud Architect,Super Micro Computer,"San Jose, California","Job Req ID: 27871

About Supermicro:

Supermicro® is a Top Tier provider of advanced server, storage, and networking solutions for Data Center, Cloud Computing, Enterprise IT, Hadoop/ Big Data, Hyperscale, HPC and IoT/Embedded customers worldwide. We are the #5 fastest growing company among the Silicon Valley Top 50 technology firms. Our unprecedented global expansion has provided us with the opportunity to offer a large number of new positions to the technology community. We seek talented, passionate, and committed engineers, technologists, and business leaders to join us.

Job Summary:

We seek a Senior Cloud Architect to own full-lifecycle delivery of SuperCloud Automation Center (SCAC) software orchestrator solutions. In this role, you will translate business and technical requirements into secure, scalable architectures; standardize patterns; identify and mitigate risks; and guide customers from discovery and POC through production. Success requires strong program management, deep infrastructure/cloud automation expertise, and crisp communication across teams.

Essential Duties and Responsibilities:

Includes the following essential duties and responsibilities (other duties may also be assigned):
• Leading architecture design for cloud-based orchestration and provisioning solutions
• Evaluating new technologies and rapidly developing proof-of-concepts that can be taken to production ready products
• Own end-to-end solution delivery for SCAC discovery, architecture, implementation, and production readiness
• Create and socialize architectural artifacts in the form of HLD/LLD, integration diagrams, data flows, and ADR’s
• Design integration patterns across Supermicro Systems Management Software, Kubernetes, OpenShift, VMware, storage, network, and CI/CD
• Identify and close design gaps and risks while proposing mitigation options with clear trade-offs and TCO impact
• Establish and enforce standards in the form of reference architectures, golden paths, and IaC modules
• Champion security-by-design with zero-trust principles, secrets management, vulnerability scanning, and compliance guardrails
• Define orchestration workflows for cluster bring-up, upgrades, patching, and teardown
• Specify installer strategy and UX, including preflight checks, image registry, mirror configuration, repository layout, admin node, IP addressing, and hypervisor flags.
• Formalize integration contracts, API contracts, API’s with SCAC, telemetry, logging, and external registries
• Lead performance, scalability, modeling, and validation to include, but not limited to, concurrency limits, queue backoff, and artifact caching
• Guide delivery execution from design to GA, acceptance criteria, risk registers, exit gates, and sign-offs for security, scale, and operability
• Validate the end-to-end solutions in reference labs and customer environments, ensuring upgrade and rollback readiness, and versioning and packing strategy

Qualifications:

• BS in Computer Science, Computer/Software Engineering, Electrical Engineering, or related field
• Architectural experience 8 years in a formal architect role, solutions, systems, and software owning HLD and LLD with cross-team design inputs
• Strong Python coding, analysis, debugging and problem solving
• Hands on Linux development, Docker, Containers, plus experience with OpenShift or vanilla K8s with understanding controllers, operators, and multi-clusters
• Experience in Git based development, SQL or NoSQL databases ( e.g. PostgreSQL), Restful API design.
• Security fundamentals, zero-trust principles, image signing, and SBOM basics in vulnerability management
• Experience in architecting end-to-end solutions, systems management framework for private cloud-based offerings
• Excellent communication, documentation skills

Salary Range

​$180,000 - $200,000

The salary offered will depend on several factors, including your location, level, education, training, specific skills, years of experience, and comparison to other employees already in this role. In addition to a comprehensive benefits package, candidates may be eligible for other forms of compensation, such as participation in bonus and equity award programs.

​

EEO Statement

Supermicro is an Equal Opportunity Employer and embraces diversity in our employee population. It is the policy of Supermicro to provide equal opportunity to all qualified applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran stat",https://jobs.supermicro.com/job/San-Jose-Sr_-Cloud-Architect-Cali/1340240000/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic,JSearch,True,"The job posting does not contain any explicit or implicit restrictions regarding visa sponsorship or work authorization. There are no statements indicating that only U.S. citizens or permanent residents are eligible, nor does it mention that no sponsorship is available. Therefore, H1B visa holders are likely eligible for this position.",2025-12-28 12:44
"Data Engineer/Tech Data Architect - Owings Mills, MD",Georgia Tek Systems,"Owings Mills, Maryland","Title - Data Engineer/Tech Data Architect
Location - Owings Mills, MD
Rate - DOE
Start Date - Immediately

Detailed JD
Looking for a Data Engineer/Tech Data Architect specializing in Hands on data engineering work for our technology & business partners. You will be part of the Distribution & Marketing Solutions Architecture team and steer strategic technology direction, define target state architecture, roadmaps and help build reference implementations in partnership with our Platform Engineering teams.
Tech Architect Job Responsibilities
• At least 5+ years' experience as a data engineer
• Experience in Adobe Experience Platform Integration patterns
• Experience in building framework and reference Implementations
• Experience in SQL is a must
• Experience coding in python, pyspark for server side/data processing
• 2+ years' experience using modern data stack (spark, snowflake) on cloud platforms (AWS)
• Experience building ETL/ELT pipelines for complex data engineering projects (using Airflow, dbt, Great Expectations would be a plus)
• Experience with Database Modeling, Normalization techniques
• Experience with dev ops tools like Git, Jenkins, Gitlab CI
• Skills that would be a plus:
• ETL tools (Informatica, Snaplogic, dbt etc.)
• Experience with Snowflake or other Cloud Data warehousing products
• Exposure with Workflow management tools such as Airflow
• Exposure to messaging platforms such as Kafka
• Exposure to New SQL platforms such as Cockroachdb, Postgres etc.

Skills:
Adobe Product Family, Amazon Web Services (AWS), Apache Kafka, Cloud Computing, Data Processing, Data Warehousing, DataArchitect Data Modeling Tool, Database Design, Database Extract Transform and Load (ETL), Engineering, Git, Informatica, Jenkins, Marketing Software, Python Programming/Scripting Language, SQL (Structured Query Language), Snowflake Schema, Strategic Planning, Technical Strategy, United States Department of Energy (DOE)

About the Company:
Georgia Tek Systems",https://www.monster.com/job-openings/data-engineer-tech-data-architect-owings-mills-md-owings-mills-md--6e907cf1-bccf-450a-9ad8-f64a0c45d01b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic,JSearch,True,"The job posting does not contain any explicit or implicit restrictions regarding visa sponsorship or work authorization. There are no statements indicating that only U.S. citizens or permanent residents are eligible, nor does it mention that no visa sponsorship is available. Therefore, H1B visa holders are likely eligible for this position.",2025-12-28 12:44
Solution Architect – Data/ 6months,PGC Digital (America) Inc: CMMI Level 3 Company,"Phoenix, Arizona","Overview:

The Solution Architect – Data is responsible for contributing to the design, modernization, and optimization of enterprise-scale data systems, as well as the maintenance and operations strategy for CHP. This role involves designing and implementing data systems that organize, store, and manage data within our cloud data platform.

The architect will perform continuous maintenance and operations work for CHP in the cloud environment. They will review and analyze CHP’s data infrastructure, plan future database solutions, and implement systems to support data management for CHP users.

Additionally, this role is accountable for ensuring data integrity, making sure the CHP team adheres to data governance standards to maintain accuracy, consistency, and reliability across all systems. The architect will identify data discrepancies and quality issues, and work to resolve them.

This position requires a strong blend of architectural leadership, technical depth, and the ability to collaborate with business stakeholders, data engineers, machine learning practitioners, and domain experts to deliver scalable, secure, and reliable AI-driven solutions.

The ideal candidate will have a proven track record of delivering end-to-end ETL/ELT pipelines across Databricks, Azure, and AWS environments.

Key Responsibilities:

• Design scalable data lake and data architectures using Databricks and cloud-native services.

• Develop metadata-driven, parameterized ingestion frameworks and multi-layer data architectures.

• Optimize data workloads and performance.

• Define data governance frameworks for CHP.

• Design and develop robust data pipelines.

• Architect AI systems, including RAG workflows and prompt engineering.

• Lead cloud migration initiatives from legacy systems to modern data platforms.

• Provide architectural guidance, best practices, and technical leadership across teams.

• Build documentation, reusable modules, and standardized patterns.

Required Skills and Experience:

• Strong expertise in cloud platforms, primarily Azure or AWS.

• Hands-on experience with Databricks.

• Deep proficiency in Python and SQL.

• Expertise in building ETL/ELT pipelines and ADF workflows.

• Experience architecting data lakes and implementing data governance frameworks.

• Hands-on experience with CI/CD, DevOps, and Git-based development.

• Ability to translate business requirements into technical architecture.

Technical Expertise:

Programming: Python, SQL, R

Big Data: Hadoop, Spark, Kafka, Hive

Cloud Platforms: Azure (ADF, Databricks, Azure OpenAI), AWS

Data Warehousing: Redshift, SQL Server

ETL/ELT Tools: SSIS

Required Educational Background:

• Bachelor’s degree in Computer Science, Information Technology, Information Systems, Engineering, or a related field.

• 6+ years of experience in data engineering or .NET development.",https://www.linkedin.com/jobs/view/solution-architect-%E2%80%93-data-6months-at-pgc-digital-america-inc-cmmi-level-3-company-4346101069?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic,JSearch,True,"The job posting does not contain any explicit or implicit restrictions regarding visa sponsorship or work authorization. There are no statements indicating that only U.S. citizens or Green Card holders are eligible, nor does it mention that visa sponsorship is unavailable. Therefore, it can be inferred that H1B visa holders may apply for this position.",2025-12-28 12:44
Staff Data Engineer – Cloud Data Platform,01 Calix North America,"Remote, Oregon","Please note that all emails from Calix will come from a @calix.com email address. If you receive a communication that you think may not be from Calix, please report it to us at talentandculture@calix.com. This is a remote position that could be based anywhere in the United States. Calix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks. As part of a high performing global team, the right candidate will play a significant role as Calix Cloud Data Engineer involved in architecture design, implementation, technical leadership in data ingestion, extraction, transformation and analytics. Responsibilities and Duties: Work closely with Cloud product owners to understand, analyze product requirements and provide feedback. Develop conceptual, logical, physical models and meta data solutions. Design and manage an array of data design deliverables including data models, data diagrams, data flows and corresponding data dictionary documentations. Determine database structural requirements by analyzing client operations, applications, and data from existing systems. Technical leadership of software design in meeting requirements of service stability, reliability, scalability, and security Guiding technical discussions within engineer group and making technical recommendations. Design review and code review with peer engineers Guiding testing architecture for large scale data ingestion and transformations. Customer facing engineering role in debugging and resolving field issues. Qualifications: This role may be required to travel and attend face-to-face meetings and Calix sponsored events. 10+ years of development experience performing Data modeling, master data management and building ETL/data pipeline implementations. Cloud Platforms: Proficiency in both Google Cloud Platform (GCP) services (BigQuery, Dataflow, Dataproc, PubSub/Kafka, Cloud Storage) and AWS Big Data Technologies: Knowledge of big data processing frameworks such as Apache Spark ,Flink . Programming Languages: Strong knowledge of SQL and at least one programming language (Python, Java, or Scala),DBT. Data Visualization: Experience with BI tools such as Google Data Studio, Looker, ThoughtSpot, and using BigQuery BI Engine for optimized reporting Problem Solving: Strong analytical and troubleshooting skills, particularly in complex data scenarios. Collaboration: Ability to work effectively in a team environment and engage with cross-functional teams. Communication: Proficient in conveying complex technical concepts to stakeholders. Knowledge of data governance, security best practices, and compliance regulations in both GCP and AWS environments. Bachelor’s degree in Computer Science, Information Technology or a related field. Location: Remote-based position located in the United States. #LI-Remote The base pay range for this position varies based on the geographic location. More information about the pay range specific to candidate location and other factors will be shared during the recruitment process. Individual pay is determined based on location of residence and multiple factors, including job-related knowledge, skills and experience. San Francisco Bay Area: 156,400 - 265,700 USD Annual All Other US Locations: 136,000 - 231,000 USD Annual As a part of the total compensation package, this role may be eligible for a bonus. For information on our benefits click here. PLEASE NOTE: All emails from Calix will come from a '@calix.com' email address. Please verify and confirm any communication from Calix prior to disclosing any personal or financial information. If you receive a communication that you think may not be from Calix, please report it to us at talentandculture@calix.com. Calix delivers a broadband platform and managed services that enable our customers to improve life one community at a time. We’re at the forefront of a once in a generational change in the broadband industry. Join us as we innovate, help our customers reach their potential, and connect underserved communities with unrivaled digital experiences. This is the Calix mission - to enable BSPs of all sizes to Simplify. Innovate. Grow. To learn more, visit the Calix web site at www.calix.com To learn more about our international job opportunities, please visit our International Careers Page If you are a person with a disability needing assistance with the application process please: Email us at calix.interview@calix.com; or Call us at +1 (408) 514-3000. Calix is a Drug Free Workplace. You may access a copy of Calix Candidate Privacy Policy HERE and other Calix Privacy Policies HERE.",https://calix.wd1.myworkdayjobs.com/en-US/External/job/Staff-Data-Engineer---Cloud-Data-Platform_R-10714?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic,JSearch,True,"The job posting does not contain any explicit or implicit restrictions regarding visa sponsorship or work authorization. There are no statements indicating that only U.S. citizens or Green Card holders are eligible, nor does it mention that no sponsorship is available. Therefore, H1B visa holders are likely eligible for this position.",2025-12-28 12:44
Principal Software Engineer – Data Engineering and Cloud,"ThreeV Technologies, Inc.","Los Angeles, California","ThreeV Technologies, Inc. is an early stage venture-backed startup developing a Multi-Sensory Asset Intelligence Platform (MSAIP) to revolutionize sectors like Power & Utilities, Renewables, and Public Infrastructure. The company aims to become the B2B Perplexity AI for the physical world, utilizing frontier AI techniques, large language models, and computer vision to improve inspection processes and asset management. The role of Principal Software Engineer focuses on scaling their AI SaaS platform through cloud architecture, data pipelines, and microservices. Responsibilities include designing high-quality Python code, building data pipelines with Kafka, sensors, and data lakes, working with AWS, Azure, Kubernetes, and container-based applications, and leading technical strategy for multi-cloud infrastructure. The candidate will mentor junior engineers, participate in customer engagements, and help create reliable, low-latency data systems. The work environment is remote-first, with a primary base in Los Angeles, CA, and some travel for customer meetings. The company values mission-driven work, integrity, mastery, and diversity, aiming to solve critical infrastructure challenges with innovative AI solutions.",https://lazyapply.com/company/threev-technologies-inc/jobs/principal-software-engineer-data-engineering-and-cloud-los-angeles-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic,JSearch,True,"The job posting does not contain any explicit or implicit restrictions against H1B visa holders, such as requirements for permanent US work authorization or statements about not providing visa sponsorship. There are also no indications that only US citizens or Green Card holders are eligible. Therefore, it can be inferred that H1B visa holders are eligible for this position.",2025-12-28 12:44
